# Configuration for backend when using llama.cpp
BASE_URL=http://llama-cpp:8080/v1/
MODEL=ai/llama3.2:1B-Q8_0
API_KEY=${API_KEY:-llamacpp}

# Observability configuration
LOG_LEVEL=info
LOG_PRETTY=true
TRACING_ENABLED=true
OTLP_ENDPOINT=jaeger:4318
