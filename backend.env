BASE_URL=http://model-runner.docker.internal/engines/llama.cpp/v1/
MODEL=ai/llama3.2:1B-Q8_0
API_KEY=${API_KEY:-dockermodelrunner}

# Observability configuration
LOG_LEVEL=info
LOG_PRETTY=true
TRACING_ENABLED=true
OTLP_ENDPOINT=jaeger:4318

# llama.cpp observability settings
LLAMACPP_METRICS_ENABLED=true
LLAMACPP_BASE_URL=http://model-runner.docker.internal/engines/llama.cpp/v1
LLAMACPP_EXPORTER_URL=http://llamacpp-exporter:9100/metrics